{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immigrant and Temperature Facts Exploration - An ETL Pipeline\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The main taget of the project is to create a data pipeline built up using the immigrants data and the temperature data. We need to create a ELT database which is well optimized for running queries and performing other analytical operation to gather the facts/insights from this data.Using this database we can analyze the temperature behaviour of the different locations of the immigrant's destination.\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import udf,col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scope**\n",
    "\n",
    "<font size=\"3\">\n",
    "    For this Capstone project, I am planning to take the Immigration data which consists of the immigrants personal details e.g.  when they arrived, what was the source city, and where what's their destination city in USA. I am planning to take the temperature dataset from the kaggle and combining it with this immigration dataset. We can aggregate this data on the basis of the <strong><i>destination city</i></strong> in USA. We can analyze the temperature behaviour on the basis of that particular city and how it affects the immigrant's destination city.To deal with this large amount of data we will be using spark as our data analytics tool for processing.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe and Gather Data\n",
    "<br/>\n",
    "<div>\n",
    "<strong> Dataset Name : Immigration Data</strong>\n",
    "<br>\n",
    "<font size=\"3\">\n",
    "    This data comes from the <a href=\"https://travel.trade.gov/research/reports/i94/historical/2016.html\">US National Tourism and Trade Office</a>. This data contains the immigrant details which have been immigrated to USA. The description of the dataset is provided below:<br />\n",
    "        <em>Dataset Format</em> : \"SAS7BDAT\" (A binary format)\n",
    "\n",
    "<br />\n",
    "    <table>\n",
    "        <thead>\n",
    "            <td>Field Name</td>\n",
    "            <td>Field Description</td>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>i94yr</td>\n",
    "                <td> Year (Numeric)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>i94mon</td>\n",
    "                <td>Month (Numeric)</td>\n",
    "            </tr><tr>\n",
    "                <td>i94city</td>\n",
    "                <td>A 3 digit code for origin city(Numeric)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>i94res</td>\n",
    "                <td>A 3 digit code(Numeric)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>i94port</td>\n",
    "                <td>Destination City Code (3 Character)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>arrdate</td>\n",
    "                <td>Arrival date (in USA)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>i94mode</td>\n",
    "                <td>Travel Code (Numeric)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>i94addr</td>\n",
    "                <td>USA city code</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>depdate</td>\n",
    "                <td>The departure date</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>i94visa</td>\n",
    "                <td>Reason of Immigration (Numeric)</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "    <br />\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<strong> Dataset Name : Temperature Data</strong>\n",
    "<br>\n",
    "<font size=\"3\">\n",
    "    This data comes from the <a href=\"https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\">Kaggle</a>. This data contains the temperature details of a particular city. The description of the dataset is provided below:<br />\n",
    "        <em>Dataset Format</em> : \"CSV\" (Comma Seperated values)\n",
    "\n",
    "<br />\n",
    "    <table>\n",
    "        <thead>\n",
    "            <td>Field Name</td>\n",
    "            <td>Field Description</td>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>dt</td>\n",
    "                <td> Date when temperature is measured</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>AverageTemperature</td>\n",
    "                <td>Average temperature of the city</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>City</td>\n",
    "                <td>Name of the city</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Country</td>\n",
    "                <td>Country where city is located</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Latitude</td>\n",
    "                <td>The latitude</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Longitude</td>\n",
    "                <td>The Longitude</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "    <br />\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining all the properties that are being used in the project\n",
    "props = {\n",
    "    'immDataPath':'../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "    'tempDataPath':'../../data2/GlobalLandTemperaturesByCity.csv',\n",
    "    'immDestCodeMappingPath':'imm_dest_mapping.csv',\n",
    "    'outputBasePath':'/output'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/sas/sas7bdat.py\u001b[0m in \u001b[0;36m_read_next_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrslt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_page_data_subheader_pointers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path_or_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_page_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reading the immigration data from the path\n",
    "immigrationDf = pd.read_sas(props['immDataPath'], 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigrationDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Kaggle's temperature data\n",
    "tempDataPathDf = pd.read_csv(props['tempDataPath'], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDataPathDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "#Creating a spark session and loading the immigrant data\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load(props['immDataPath'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning Steps\n",
    "\n",
    "We have extracted the destination mapping data from the \"I94_SAS_Labels_Description.SAS\". We are converting it to dictionary to generate key value mapping for each and every destination code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing the Destination city codes for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "destination_df = pd.read_csv(props['immDestCodeMappingPath'],sep='=',names=['dest_code','dest'])\n",
    "destination_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_df['city'] = destination_df.apply(lambda x : x.dest.split(',')[0].strip(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_mapping = dict(zip(destination_df.dest_code,destination_df.city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to filter immigration data by destination codes\n",
    "def filter_data_by_destcode(sparkSession, input_path,key_mapping):\n",
    "    '''  \n",
    "    Filters the i94 input dataframe containing rows with valid destination codes. \n",
    "    \n",
    "    :param inputPath: The input path for which dataset needs to be filtered\n",
    "    :param key_mapping: The key value mapping of the destination codes and destination name\n",
    "    :return: The filtered dataframe\n",
    "    '''    \n",
    "    # Read the data with spark session\n",
    "    df = sparkSession.read.format('com.github.saurfang.sas.spark').load(input_path)\n",
    "\n",
    "    # Filter out entries where i94port is not present\n",
    "    df = df.filter(df.i94port.isin(list(key_mapping.keys())))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark dataframe from the input temperature data\n",
    "temp_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(props['tempDataPath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the null data and dropping the duplicates containing for columns 'City' and 'Country'\n",
    "temp_df = temp_df.filter(temp_df.AverageTemperature != 'NaN').filter(temp_df.City != 'NaN')\n",
    "temp_df = temp_df.dropDuplicates(['City', 'Country'])\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieves the reverse mapping Destination Name -> Destination Code for Optimization\n",
    "dest_code_mapping = dict([(value, key) for key, value in dest_mapping.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A spark udf\n",
    "@udf()\n",
    "def get_dest_code(city_name):\n",
    "    '''  \n",
    "    Retrieves the destination code from the city name \n",
    "    \n",
    "    :param city_name: Name of the city which we need the actual value\n",
    "    :param key_mapping: The key value mapping of the destination codes and destination name\n",
    "    :return: The destination name\n",
    "    ''' \n",
    "    for key in dest_code_mapping.keys():\n",
    "        if (city_name.strip().lower() == key.strip().lower()):\n",
    "            return dest_code_mapping[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new column in the dataframe containing the destination code\n",
    "temp_df = temp_df.withColumn(\"destCode\", get_dest_code(temp_df.City))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data points where the destination code is null\n",
    "temp_df = temp_df.filter(temp_df.destCode != 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "temp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Immigrant Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean I94 immigration data and store as Spark dataframe\n",
    "imm_df = filter_data_by_destcode(spark,props['immDataPath'],dest_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A spark udf\n",
    "@udf(StringType())\n",
    "def get_visa_purpose(visa_code):\n",
    "    '''  \n",
    "    Retrieves the visa purpose from the visa_code. \n",
    "    \n",
    "    :param visa_code: The input path for which dataset needs to be filtered\n",
    "    :return: The visa purpose\n",
    "    ''' \n",
    "    visa_code = int(visa_code)\n",
    "    purpose = 'NaN'\n",
    "    if(visa_code == 1):\n",
    "        purpose = 'Business'\n",
    "    elif (visa_code == 2):\n",
    "        purpose = 'Pleasure'\n",
    "    elif (visa_code ==3):\n",
    "        purpose = 'Student'\n",
    "    return purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A spark udf\n",
    "@udf(StringType())\n",
    "def get_travel_mode(travel_code):\n",
    "    '''  \n",
    "    Retrieves the travel_mode_type from the travel_code. \n",
    "    \n",
    "    :param travel_code: The travel_code for which travel_mode_type needs to be retrieved\n",
    "    :return: The travel_mode_type\n",
    "    ''' \n",
    "    travel_mode = 'Not reported'\n",
    "    try:\n",
    "        travel_code = int(travel_code)\n",
    "        if(travel_code == 1):\n",
    "            travel_mode = 'Air'\n",
    "        elif (travel_code == 2):\n",
    "            travel_mode = 'Sea'\n",
    "        elif (travel_code ==3):\n",
    "            travel_mode = 'Land'\n",
    "    except:\n",
    "        print(f\"Invalid travel code {travel_code}\")\n",
    "        return travel_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_df = imm_df.withColumn('i94visa',get_visa_purpose(imm_df.i94visa))\n",
    "imm_df = imm_df.withColumn('i94mode',get_travel_mode(imm_df.i94mode))\n",
    "imm_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **The Fact Table** - This will contain information from the I94 immigration data joined with the city temperature data on i94port\n",
    "\n",
    "\n",
    "Column Details:\n",
    "- immTempId : The unique id for the fact table\n",
    "- city_code : The destination city code for the immigrants\n",
    "- city : The city name referring to the city code\n",
    "- immigrant_arrival_date : Arrival date of immigrants on the destination city \n",
    "- month : The immigration month\n",
    "- year: The immigration year\n",
    "- immigrant_departure_date : Departure date of immigrants\n",
    "- travel_mode : Mode of travel\n",
    "- visa_purpose : The visa purpose\n",
    "- visa_type : The visa type\n",
    "- temperature : The average temperature of destination city.\n",
    "- latitude : The latitude coordinates of destination city\n",
    "- longitude : The longitude coordinates of destination city\n",
    "\n",
    "\n",
    "##### Dimension Table 1 - Data reffering from I94 immigration data.\n",
    "\n",
    "Column Details:\n",
    "- immigrantId : The unique id for the immigrant\n",
    "- i94port : 3 character city code\n",
    "- i94cit : 3 digit origin city code\n",
    "- arrdate : the arrival date\n",
    "- i94mon : immigration month\n",
    "- i94yr : immigration year\n",
    "- depdate : departure date\n",
    "- i94mode : mode of travel\n",
    "- i94visa : visa purpose \n",
    "- visatype : visa type\n",
    "\n",
    "##### Dimension Table 2 - Data containing city temperature information .\n",
    "Column Details:\n",
    "- tempId : The unique id for a particular city's temperature data\n",
    "- destCode : The  destination code for particular city\n",
    "- AverageTemperature : Average temperature of the city\n",
    "- City : The city reffering to the destCode\n",
    "- Country : Country where city is located\n",
    "- Latitude : The latitude coordinates of city\n",
    "- Longitude : The longitude coordinates of city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are following the below steps to pipeline the data to the data model:\n",
    "- First we are creating the dataframes for both immigrant and temperature data using spark session and input data paths.\n",
    "- Then we are cleaning the temperature data and filtering out the required events which are related to the immigrant's city.\n",
    "- Then we are cleaning the immigrant data, and mapping the required values from their codes.\n",
    "- Then we are creating the dimension tables with the required schema by selecting specific fields from dataframe and writing them in parquet format (ELT format) in the '/output' directory.\n",
    "- Then we are creating our fact table by joining these both dimension table which creates the required insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating data model for immigration table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a unique id to the immigrant dataframe\n",
    "imm_df = imm_df.withColumn('immigrantId', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns for immigration dimension table\n",
    "imm_select_cols = ['immigrantId','i94port','i94cit','arrdate','i94mon','i94yr','depdate','i94mode','i94visa','visatype']\n",
    "imm_table = imm_df.select(imm_select_cols).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing immigration dataframe (dim table) partitioned by i94port\n",
    "imm_table.write \\\n",
    "        .partitionBy('i94port') \\\n",
    "        .format('parquet') \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(props['outputBasePath'] + '/immigration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data model for Temperature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a unique id to the temperature dataframe\n",
    "temp_df = temp_df.withColumn('tempId', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns for temperature dimension table\n",
    "temp_select_cols = ['tempId','destCode','AverageTemperature','City','Country','Latitude','Longitude']\n",
    "temp_table = temp_df.select(temp_select_cols).distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write temperature dimension table to parquet files partitioned by destCode\n",
    "temp_table.write \\\n",
    "        .partitionBy('destCode') \\\n",
    "        .format('parquet') \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(props['outputBasePath'] + '/temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating fact table using the dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining tables by destination code\n",
    "imm_temp_df = imm_df.join(temp_df, imm_df.i94port == temp_df.destCode)\n",
    "imm_temp_df = imm_temp_df.withColumn('immTempId', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_temp_table = imm_temp_df.select(\n",
    "    'immTempId',\n",
    "    col('i94port').alias('city_code'),\n",
    "    col('i94cit').alias('city'),\n",
    "    col('arrdate').alias('immigrant_arrival_date'),\n",
    "    col('i94mon').alias('month'),\n",
    "    col('i94yr').alias('year'),\n",
    "    col('depdate').alias('immigrant_departure_date'),\n",
    "    col('i94mode').alias('travel_mode'),\n",
    "    col('i94visa').alias('visa_purpose'),\n",
    "    col('visatype').alias('visa_type'),\n",
    "    col('AverageTemperature').alias('temperature'),\n",
    "    col('Latitude').alias('latitude'),\n",
    "    col('Longitude').alias('longitude')\n",
    ").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_temp_table.write \\\n",
    "        .partitionBy('city_code') \\\n",
    "        .format('parquet') \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(props['outputBasePath'] + '/city_temp_facts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to perform data quality checks\n",
    "def perform_quality_check(df, table_name):\n",
    "    '''\n",
    "    Computes the quality check and logs the result for input dataframe.\n",
    "    \n",
    "    :param df: Input spark dataframe\n",
    "    :param table_name: The table name reffering to the dataframe printing data quality check\n",
    "    :return: None\n",
    "    '''\n",
    "    print(f\"===========Data Quality Check started for {table_name} table==============\")\n",
    "    rowCount = df.count()\n",
    "    \n",
    "    if rowCount == 0:\n",
    "        print(f\"Data quality check failed for {table_name} with zero records\")\n",
    "    else:\n",
    "        print(f\"Data quality check passed for {table_name} with {rowCount} records\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data quality checks on fact and dimension tables\n",
    "perform_quality_check(imm_df, 'immigration')\n",
    "perform_quality_check(temp_df, 'temperature')\n",
    "perform_quality_check(imm_temp_df,'city_temp_facts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Data dictionary\n",
    "##### **The Fact Table** - This will contain information from the I94 immigration data joined with the city temperature data on i94port\n",
    "\n",
    "\n",
    "Column Details:\n",
    "- immTempId : The unique id for the fact table\n",
    "- city_code : The destination city code for the immigrants\n",
    "- city : The city name referring to the city code\n",
    "- immigrant_arrival_date : Arrival date of immigrants on the destination city \n",
    "- month : The immigration month\n",
    "- year: The immigration year\n",
    "- immigrant_departure_date : Departure date of immigrants\n",
    "- travel_mode : Mode of travel\n",
    "- visa_purpose : The visa purpose\n",
    "- visa_type : The visa type\n",
    "- temperature : The average temperature of destination city.\n",
    "- latitude : The latitude coordinates of destination city\n",
    "- longitude : The longitude coordinates of destination city\n",
    "\n",
    "The schema details for the fact table are described below - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dimension Table 1 - Data reffering from I94 immigration data.\n",
    "\n",
    "Column Details:\n",
    "- immigrantId : The unique id for the immigrant\n",
    "- i94port : 3 character city code\n",
    "- i94cit : 3 digit origin city code\n",
    "- arrdate : the arrival date\n",
    "- i94mon : immigration month\n",
    "- i94yr : immigration year\n",
    "- depdate : departure date\n",
    "- i94mode : mode of travel\n",
    "- i94visa : visa purpose \n",
    "- visatype : visa type\n",
    "\n",
    "The schema details for the immigration dimension table are described below - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dimension Table 2 - Data containing city temperature information .\n",
    "Column Details:\n",
    "- tempId : The unique id for a particular city's temperature data\n",
    "- destCode : The  destination code for particular city\n",
    "- AverageTemperature : Average temperature of the city\n",
    "- City : The city reffering to the destCode\n",
    "- Country : Country where city is located\n",
    "- Latitude : The latitude coordinates of city\n",
    "- Longitude : The longitude coordinates of city\n",
    "\n",
    "The schema details for the immigration temperature table are described below - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "I have used the spark for this project due to below reasons:\n",
    "- We can easily manage input data from various for different file formats eg. SAS,csv etc.\n",
    "- Its fast, easy and effecient to process the large amount of data\n",
    "- Even if the data or number of users increases we can reuse the same code by modifying the spark configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose how often the data should be updated and why.\n",
    "As the raw files are being formatted by month, we can update the data on monthy basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can manage the problem differently considering all the above scenarios:\n",
    "\n",
    "\n",
    "<strong>Scenario 1: The data was increased by 100x </strong><br>\n",
    "<font size=\"3\">\n",
    "Solution: We can load our data into Amazon Redshift. Its highly optimized for large workloads and operations like aggeregating data.\n",
    "</font>\n",
    "<br/><br/>\n",
    "<strong>Scenario 2: The data populates a dashboard that must be updated on a daily basis by 7am every day.</strong><br>\n",
    "<font size=\"3\">\n",
    "    Solution: This action can be easily performed using <b> Airflow</b> by creating a pipeline and adding scheduler to it which will run on daily basis. We can also add DAG retry mechanism, perform data checks, and we can also send emails in case of failures. We can also alter the dashboard functionality in case of failures\n",
    "</font>\n",
    "<br/><br/>\n",
    "<strong>Scenario 3: The database needed to be accessed by 100+ people.</strong><br>\n",
    "<font size=\"3\">\n",
    "Solution: This also can be achieved by using Amazon Redshift.Its completely auto scalable, highly optimized with extremely good performance during any operations performed.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
